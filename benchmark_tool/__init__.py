"""
Benchmark Tool for comparing a code generated by LLMs with a human reference 
in a certain domain - Digital Signal Processing in this case.

A package for generating, testing, and evaluating code generated by LLMs.
Provides tools for:
- Code generation using LLMs
- Automated testing of generated code
- Metrics calculation and comparison
- End-to-end benchmarking pipeline
"""

from benchmark_tool.generator import CodeGenerator
from benchmark_tool.tester import CodeTester 
from benchmark_tool.metrics import CodeMetrics
from benchmark_tool.runner import BenchmarkRunner

# Version information
__version__ = "0.1.0"
__author__ = "Vladislav Bulgakov"

# Explicitly define what should be available when using "from benchmark_tool import *"
__all__ = [
    "CodeGenerator",
    "CodeTester", 
    "CodeMetrics",
    "BenchmarkRunner"
]